{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootf = FOLDER ## place here the folder where you save PGM, Align_utils and the other files ##\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "curr_float = np.float32\n",
    "curr_int = np.int16\n",
    "\n",
    "def convert_number(seqs): # convert to numbers already aligned seqs\n",
    "    aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',  'W', 'Y','-']\n",
    "    aadict = {aa[k]: k for k in range(len(aa))}\n",
    "    msa_num = np.array(list(map(lambda x: [aadict[y] for y in x], seqs[0:])), dtype=curr_int, order=\"c\")\n",
    "    \n",
    "    return msa_num\n",
    "\n",
    "def convert_letter(seqs_n): # convert to numbers already aligned seqs\n",
    "    aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',  'W', 'Y','-']\n",
    "    aadictinv = {k: aa[k] for k in range(len(aa))} \n",
    "    seqs=[]\n",
    "    if type(seqs_n[0]) == curr_int:\n",
    "        seqs.append(''.join([aadictinv[e] for e in seqs_n]))\n",
    "    else:\n",
    "        for t in range(len(seqs_n)):\n",
    "            seqs.append(''.join([aadictinv[e] for e in seqs_n[t]]))\n",
    "    return seqs\n",
    "\n",
    "#%matplotlib inline\n",
    "import sys, os, pickle\n",
    "sys.path.append(rootf + '/PGM/source/')\n",
    "sys.path.append(rootf + '/PGM/utilities/')\n",
    "sys.path.append(rootf + '/Align_utils/')\n",
    "from common_imports import set_num_threads\n",
    "set_num_threads(1) # Set the number of cores. Must be executed before importing numpy&numba.\n",
    "import rbm,utilities\n",
    "import Proteins_utils, RBM_utils, utilities, sequence_logo, plots_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read tables with counts before and after antigen stimulation ##\n",
    "table_data = pd.read_csv(rootf + '/example_file.tsv', sep='\\t', low_memory=False)\n",
    "        \n",
    "seqs = list(table_data['CDR3'])\n",
    "mult_seqs_post = table_data['Count post-stimulation'].values\n",
    "mult_seqs_pre = table_data['Count pre-stimulation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block is to align the sequences##\n",
    "\n",
    "import subprocess\n",
    "name_mat = rootf + '/Align_utils/align_protpy.py'\n",
    "\n",
    "SA = 19\n",
    "SAmin = 5\n",
    "SAmax = 23\n",
    "\n",
    "all_seqs1 = seqs\n",
    "\n",
    "name_seed = rootf + '/Align_utils/prots_seed.txt'\n",
    "with open(name_seed, 'w') as out_f:\n",
    "    for u in range(len(all_seqs1)):\n",
    "        if len(all_seqs1[u]) >= 5:  \n",
    "            out_f.write(all_seqs1[u] + '\\n')\n",
    "\n",
    "subprocess.call('python3 ' + name_mat + ' -ss ' + name_seed + ' -SA ' + str(SA) + ' -SAmin ' + str(SAmin) + ' -SAmax ' + str(SAmax), shell = True)\n",
    "\n",
    "seqs_al = np.loadtxt(rootf + '/Align_utils/aligned_prot.txt')\n",
    "sn = seqs_al.astype(np.int)\n",
    "seqs_al = convert_letter(sn) ## this is the aligned dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## produce count weighted datasets, i.e. datasets where each sequence is present as many times as its count pre or post stimulation ##\n",
    "import numpy.matlib \n",
    "import random\n",
    "\n",
    "seqs_post_training = []\n",
    "for u in range(len(seqs_al)):\n",
    "    repl = np.matlib.repmat(seqs_al[u], mult_seqs_post[u], 1).tolist()\n",
    "    for y in repl:\n",
    "        seqs_post_training.append(y[0]) \n",
    "        \n",
    "seqs_pre_training = []\n",
    "for u in range(len(seqs_al)):\n",
    "    repl = np.matlib.repmat(seqs_al[u], mult_seqs_pre[u], 1).tolist()\n",
    "    for y in repl:\n",
    "        seqs_pre_training.append(y[0]) \n",
    "\n",
    "random.shuffle(seqs_post_training)\n",
    "random.shuffle(seqs_pre_training)\n",
    "\n",
    "filename = rootf + '/example_file_post.fasta'\n",
    "sequences = seqs_post_training\n",
    "\n",
    "all_labels = ['S%s'%k for k in range(len(sequences))]\n",
    "with open(filename,'w') as fil:\n",
    "    for seq, label in zip(sequences,all_labels):\n",
    "        fil.write('>%s\\n'%label)\n",
    "        fil.write('%s\\n'%seq)\n",
    "        \n",
    "filename = rootf + '/example_file_pre.fasta'\n",
    "sequences = seqs_pre_training\n",
    "\n",
    "all_labels = ['S%s'%k for k in range(len(sequences))]\n",
    "with open(filename,'w') as fil:\n",
    "    for seq, label in zip(sequences,all_labels):\n",
    "        fil.write('>%s\\n'%label)\n",
    "        fil.write('%s\\n'%seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here I select load the sample in FASTA format I have just produced ##\n",
    "\n",
    "path_in = rootf  ## simply the folder where the CDR3 sample is saved - note it is a count-weighted dataset of aligned sequences  \n",
    "\n",
    "TS=80 ## percentage of the dataset to learn the model ##\n",
    "\n",
    "# Load Data\n",
    "filename_post = '/example_file_post.fasta'\n",
    "all_data_post = Proteins_utils.load_FASTA(path_in + filename_post,with_labels=False,remove_insertions=False, drop_duplicates=False)\n",
    "\n",
    "filename_pre = '/example_file_pre.fasta'\n",
    "all_data_pre = Proteins_utils.load_FASTA(path_in + filename_pre,with_labels=False,remove_insertions=False, drop_duplicates=False)\n",
    "\n",
    "## From here, I learn RBM and next manipulate the outcome ##\n",
    "n_v = 19 # Number of visible units; = # sites in alignment.\n",
    "n_h = 25 # Number of hidden units.\n",
    "l12 = 0.1\n",
    "\n",
    "# Decide the name for the output #\n",
    "namem_pre = '/model_' + str(n_h) + '_' + str(l12) + '_pre.data'\n",
    "namem_post = '/model_' + str(n_h) + '_' + str(l12) + '_post.data'\n",
    "\n",
    "## RBM training ##\n",
    "maketraining = 1 # Variable to decide whether you want to train a new model or to load an existing one \n",
    "\n",
    "if maketraining:\n",
    "    \n",
    "    ## These are parameters for the training ##\n",
    "    visible = 'Potts' # Nature of visible units potential. Here, Potts states...\n",
    "    n_cv = 21 # With n_cv = 21 colors (all possible amino acids + gap)\n",
    "    hidden = 'dReLU' # Nature of hidden units potential. Here, dReLU potential.\n",
    "    # hidden = 'Gaussian' # Nature of hidden units potential. Here, dReLU potential.\n",
    "    seed = 0 # Random seed (optional)\n",
    "    batch_size = 100 # Size of mini-batches (and number of Markov chains used). Default: 100.\n",
    "    n_iter = 200 # Number of epochs\n",
    "    learning_rate = 0.1 # Initial learning rate (default: 0.1)\n",
    "    decay_after = 0.5 # Decay learning rate after 50% of iterations (default: 0.5)\t\n",
    "    N_MC = 10 # Number of Monte Carlo steps between each update\n",
    "    l1b=l12\n",
    "    \n",
    "    B = all_data_post.shape[0]\n",
    "    RBM_post = rbm.RBM(visible = visible,hidden = hidden ,n_v = n_v,n_h = n_h, n_cv = n_cv, random_state = seed, zero_field = False)\n",
    "    l2f = 1/len(all_data_post[0:int(B/100*TS)])\n",
    "    RBM_post.fit(all_data_post[0:int(B/100*TS)], weights=None, batch_size = batch_size,n_iter = n_iter, l1b = l1b, l2_fields = l2f, N_MC = N_MC, decay_after = decay_after, verbose = 1)\n",
    "    RBM_utils.saveRBM(rootf + namem_post, RBM_post)\n",
    "    \n",
    "    B = all_data_pre.shape[0]\n",
    "    RBM_pre = rbm.RBM(visible = visible,hidden = hidden,n_v = n_v,n_h = n_h, n_cv = n_cv, random_state = seed, zero_field = False)\n",
    "    l2f = 1/len(all_data_pre[0:int(B/100*TS)])\n",
    "    RBM_pre.fit(all_data_pre[0:int(B/100*TS)], weights=None, batch_size = batch_size,n_iter = n_iter, l1b = l1b, l2_fields = l2f, N_MC = N_MC, decay_after = decay_after, verbose = 1)\n",
    "    RBM_utils.saveRBM(rootf + namem_pre, RBM_pre)\n",
    "    \n",
    "else:\n",
    "\n",
    "    RBM_pre = RBM_utils.loadRBM(namem_pre)\n",
    "    RBM_post = RBM_utils.loadRBM(namem_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model's parameters inspection ##\n",
    "\n",
    "weights = RBM_post.weights ## Interesting features are the ones picked up in the 3 weeks sample ##\n",
    "\n",
    "s2=16;\n",
    "interesting_features = [0,2,6,7,11,13,17] ## select what hidden units inspect \n",
    "for i in range(len(interesting_features)):\n",
    "    fig = sequence_logo.Sequence_logo(weights[interesting_features[i]], figsize=(5.5,1.8), ylabel = 'Weights ' + str(interesting_features[i]+1),  ticks_every=5, ticks_labels_size=s2-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model on the data##\n",
    "seqs_n = convert_number(seqs_al)\n",
    "\n",
    "ll_post = RBM_post.likelihood(seqs_n)\n",
    "ll_pre = RBM_pre.likelihood(seqs_n)\n",
    "\n",
    "## Projection onto hidden units space ##\n",
    "\n",
    "ind_sort = np.argsort(ll_post)[::-1]\n",
    "ntop = 10 ## how many sequenes to plot##\n",
    "\n",
    "I = RBM_post.vlayer.compute_output(seqs_n[ind_sort[:ntop]],RBM_post.weights)\n",
    "\n",
    "ix = 11\n",
    "iy = 10\n",
    "\n",
    "s1 = 120.0\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(6.2)\n",
    "fig.set_figheight(6)\n",
    "\n",
    "listan = [seqs[ind_sort[t]] for t in range(ntop)]\n",
    "x=I[:,ix]\n",
    "y=I[:,iy]\n",
    "\n",
    "sc = ax.scatter(x,y, s=s1, c = np.log10(mult_seqs_post[ind_sort[:ntop]]), edgecolors='k', cmap='YlGn')\n",
    "for i, txt in enumerate(listan):\n",
    "    ax.annotate(txt, (x[i]-1, y[i]+0.25), fontfamily='monospace', fontvariant='small-caps', fontsize=14)\n",
    "\n",
    "s2=18\n",
    "ax.set_xlabel('Input to ' + str(ix+1), fontsize=s2)\n",
    "ax.set_ylabel('Input to ' + str(iy+1), fontsize=s2)\n",
    "\n",
    "cbar = plt.colorbar(mappable=sc, ax=ax, shrink=0.7,orientation='horizontal')\n",
    "cbar.ax.tick_params(labelsize=s2)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize = s2)\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.yaxis.tick_right()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sortm = np.argsort(mult_seqs_post)[::-1] \n",
    "\n",
    "## The likelihood is correlated to the counts ##\n",
    "print('Correlation to count - likelihood of 100 most abundant clones')\n",
    "print(np.corrcoef(ll_post[ind_sortm[:100]], np.log10(mult_seqs_post[ind_sortm[:100]]+1/2))[0,1])\n",
    "\n",
    "## Here I show the correlation of the response score to the fold change ##\n",
    "print('Correlation to fold change - response score of 100 most abundant clones')\n",
    "print(np.corrcoef(ll_post[ind_sortm[:100]] - ll_pre[ind_sortm[:100]], np.log10(mult_seqs_post[ind_sortm[:100]]+1/2) - np.log10(mult_seqs_pre[ind_sortm[:100]]+1/2))[0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
